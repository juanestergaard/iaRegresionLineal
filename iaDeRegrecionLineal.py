# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wu49fjETWjgnYtORUDlmH-hBPJVxb-2k
"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt

#import tensorflow as tf
#import numpy as np

class MIMONet(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(MIMONet, self).__init__()
        #self.fc1 = nn.Linear(input_dim, hidden_dim)
        #self.fc2 = nn.Linear(hidden_dim, output_dim)
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, hidden_dim)  # Nueva capa oculta
        self.fc4 = nn.Linear(hidden_dim, hidden_dim)
        self.fc5 = nn.Linear(hidden_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, output_dim)

    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc3(x))
        x = torch.relu(self.fc4(x))
        x = torch.relu(self.fc5(x))   
        x = self.fc2(x)
        return x

#celsius =np.array([-40,-10,0,8,15,22,38], dtype=float)
#fahrenheit =np.array([-40,14,32,46,59,72,100], dtype=float)

# Generar datos de entrada y salida
input_dim = 2
output_dim = 2
#num_samples = 100
#X = np.random.randn(num_samples, input_dim)
#Y = np.random.randn(num_samples, output_dim)

#esta es la buenax
#X = np.array( [ [0.2, 0.5], [2.0, 0.5], [5.0, 0.5] , [10.0, 0.5], [15.0, 0.5] , [0.2, 0.6], [2.0, 0.6], [5.0, 0.6] , [10.0, 0.6], [15.0, 0.6] , [20.0, 0.6]  , [25.0, 0.6],  [2.0, 0.7], [5.0, 0.7] , [10.0, 0.7], [15.0, 0.7] , [20.0, 0.7]  , [30.0, 0.7], [0.2, 0.8], [2.0, 0.8], [5.0, 0.8] , [10.0, 0.8], [15.0, 0.8] , [20.0, 0.8]  , [30.0, 0.8], [40.0, 0.8] , [45.0, 0.8], [2.0, 0.9], [5.0, 0.9] , [10.0, 0.9], [15.0, 0.9] , [20.0, 0.9]  , [30.0, 0.9], [40.0, 0.9] , [50.0, 0.9], [60.0, 0.9] , [70.0, 0.9], [2.0, 1], [5.0, 1] , [10.0, 1], [15.0, 1] , [20.0, 1.003]  , [30.0, 1.006], [40.0, 1.01] , [50.0, 1.02], [60.0, 1.025] , [70.0, 1.03], [80.0, 1.037], [90.0, 1.04] , [100.0, 1.05] ])
#Y = np.array([[1.0], [1.0312], [1.0687], [1.1187], [1.2], [1.0], [1.025], [1.05], [1.0875], [1.125], [1.1812], [1.2344], [1.0187], [1.0375], [1.0656], [1.0875], [1.1125], [1.1937], [1.0], [1.0093], [1.025], [1.04375], [1.0625], [1.07812], [1.1125], [1.1656], [1.2], [1.0065], [1.0125], [1.0281], [1.0375], [1.05], [1.06875], [1.096], [1.11875], [1.15], [1.175], [1.0], [1.0], [1.0], [1.0], [1.0], [1.003125], [1.00625], [1.0125], [1.01875], [1.025], [1.03125], [1.0375], [1.04375], [1.05] ])
#esta es la buenay
#Y = np.array([  [1.0, 0], [1.0312, 0], [1.0687, 0], [1.1187, 0], [1.2, 0], [1.0, 0], [1.025, 0], [1.05, 0], [1.0875, 0], [1.125, 0], [1.1812, 0], [1.2344, 0], [1.0187, 0], [1.0375, 0], [1.0656, 0], [1.0875, 0], [1.1125, 0], [1.1937, 0], [1.0, 0], [1.0093, 0], [1.025, 0], [1.04375, 0], [1.0625, 0], [1.07812, 0], [1.1125, 0], [1.1656, 0], [1.2, 0], [1.0065, 0], [1.0125, 0], [1.0281, 0], [1.0375, 0], [1.05, 0], [1.06875, 0], [1.096, 0], [1.11875, 0], [1.15, 0], [1.175, 0], [1.0, 0], [1.0, 0], [1.0, 0], [1.0, 0], [1.0, 0], [1.003125, 0], [1.00625, 0], [1.0125, 0], [1.01875, 0], [1.025, 0], [1.03125, 0], [1.0375, 0], [1.04375, 0],  ])

X = np.array([[1, 0], [2, 0], [3, 0], [4, 0], [5, 0], [6, 0], [7, 0], [8, 0], [9, 0], [10, 0], [11, 0], [12, 0], [13, 0], [14, 0], [15, 0], [16, 0], [17, 0], [18, 0], [19, 0], [20, 0]])
Y = np.array([[2, 0], [4, 0], [6, 0], [8, 0], [10, 0], [12, 0], [14, 0], [16, 0], [18, 0], [20, 0], [22, 0], [24, 0], [26, 0], [28, 0], [30, 0], [32, 0], [34, 0], [36, 0], [38, 0], [40, 0]])

#capa = tf.keras.layers.Dense(units=1, input_shape=[1]) #8:00
#modelo=tf.keras.Sequential([capa])

# Entrenamiento de la red neuronal
hidden_dim = 3
net = MIMONet(input_dim, hidden_dim, output_dim)
criterion = nn.MSELoss()
optimizer = torch.optim.SGD(net.parameters(), lr=0.0005)

num_epochs = 2000
loss_history = []
for epoch in range(num_epochs):
    optimizer.zero_grad()
        # Forward pass
    output = net(torch.Tensor(X))
    loss = criterion(output, torch.Tensor(Y))
    # Backward and optimize
    loss.backward()
    optimizer.step()
    loss_history.append(loss.item())
#modelo.compile(
    #optimizer=tf.keras.optimizers.Adam(0.1),#TAZA DE APRENDIZAJE 0.1#
    #loss='mean_squared_error'
#)

# Visualización de los resultados
plt.plot(loss_history)
plt.xlabel('Época')
plt.ylabel('Error')
plt.title('Historial de pérdida')
plt.show()

Y_pred = net(torch.Tensor(X))
plt.scatter(X[:,0], Y[:,0], label='Real')
plt.scatter(X[:,0],Y_pred.detach().numpy()[:,0], label='Predicho')
#plt.scatter(Y_pred.detach().numpy()[:,0], Y_pred.detach().numpy()[:,0], label='Predicho')
plt.legend()
plt.title('Comparación de resultados')
plt.show()
# Imprimir el error en las últimas 5 épocas
print("Error en las últimas 5 épocas:")
for i in range(-5, 0):
    print("Epoca {}: {}".format(num_epochs+i+1, loss_history[i]))

#print("comenzar entrenar")
#historial= modelo.fit(celsius,fahrenheit,epochs=1000,verbose=False)#1000 vueltas
#print("termino entrenar")

#En este ejemplo, se generan datos de entrada y salida aleatorios mediante NumPy (X e Y). 
#Luego se entrena la red neuronal (net) utilizando estos datos. 
#La red tiene una capa oculta de tamaño hidden_dim y utiliza la función de activación ReLU en la capa oculta. 
#Se utiliza el optimizador de descenso de gradiente estocástico (SGD) para optimizar la función de pérdida cuadrática media (MSELoss).

#El historial de pérdida se registra y se muestra mediante Matplotlib (plt.plot),
# y los resultados de la red neuronal se comparan con los datos reales mediante una gráfica de dispersión (plt.scatter).

#Espero que esto sea útil para su proyecto.

# Guardar los parámetros entrenados en un archivo
torch.save(net.state_dict(), 'mi_modelo.pth')

#En este caso, net.state_dict() devuelve un diccionario de Python que contiene los parámetros de la red neuronal,
# que se guardan en el archivo 'mi_modelo.pth' mediante torch.save.




#import matplotlib.pyplot as plt #para analisar el error
#plt.xlabel("# epocas")
#plt.ylabel("magnitud de perdidas")
#plt.plot(historial.history["loss"])

#Para verificar qué salida produce la red neuronal para un valor de entrada específico, 
#simplemente debes pasar ese valor de entrada a la red neuronal usando el método forward() y la red neuronal producirá la salida correspondiente.

#Aquí te muestro cómo hacerlo con la red neuronal que entrenamos:

# Crear un tensor con el valor de entrada
x = torch.Tensor([11, 0])

# Pasar el tensor a través de la red neuronal
net.eval()
with torch.no_grad():
    y_pred = net(x)

# Imprimir el resultado
print('La red neuronal predice que Y es:', y_pred.tolist())

#En este caso, la red neuronal predice que Y es [1.0875020027160645, 0.0], que se acerca bastante al valor objetivo de [1.0875, 0].
# Nota que utilizamos el método tolist() para convertir el tensor resultante a una lista de Python.

#Luego, para cargar estos parámetros en una nueva instancia de la red neuronal, puede hacer lo siguiente:
# Crear una nueva instancia de la red neuronal
net2 = MIMONet(input_dim, hidden_dim, output_dim)

# Cargar los parámetros guardados en el archivo
net2.load_state_dict(torch.load('mi_modelo.pth'))

#En este caso, torch.load carga los parámetros guardados en el archivo 'mi_modelo.pth' en la nueva instancia de la red neuronal net2
# mediante el método load_state_dict. Los parámetros de net2 ahora son los mismos que los de la red neuronal original guardada.



#print("prediccion")
#resultado = modelo.predict([100.0])
#print("el resultado es " + str(resultado) + "farenheit")

#print("variables internas")
#print(capa.get_weights())